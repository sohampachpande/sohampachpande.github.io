<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Soham Pachpande</title><link>https://sohampachpande.github.io/</link><description>Recent content in Home on Soham Pachpande</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://sohampachpande.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://sohampachpande.github.io/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sohampachpande.github.io/projects/</guid><description>Publications NLPExplorer : Exploring the Universe of NLP Papers (ECIR 2020)
Webapp: nlpexplorer.org
Developed a system using shell scripting and Python to periodically mine research article metadata and PDFâ€™s from ACL Anthology, apply OCR, index papers and derive statistics such as paper topics, citation graphs and similar papers. Stored retrieved data in MongoDB and elasticsearch Developed a full-stack web application and REST API(4000+ monthly users post publication) using Flask to visualise derived statistics and open source our data with an aim to make research more accessible Lessons from Large Scale Campus Deployment (DATA 2020)</description></item><item><title/><link>https://sohampachpande.github.io/workexperience/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sohampachpande.github.io/workexperience/</guid><description>Professional Experiences Software Engineer, HSBC Technology I worked in the Payments Data Platform Team to acclerate HSBC&amp;rsquo;s Cloud Adoptation. I enjoyed learning about payment lifecycle and data governance inside a finance company. Some of my projects were :
Streaming Data pipelines using Apache Beam framework and Google Cloud Platform. These pipelines supported Payment State Management and Transaction Alert Systems Batch data pipelines using PySpark to process and archive 1 Million+ XML payment messages daily to support SWIFT transactions Error Logging Framework to track and record errors in streaming data pipelines to aid system manageability Big Table Integration - Introduced usage of Big Table to store auxiliary data and reduce system latency to ~300 ms per message Optimising existing Data Warehouse by data type inference and partitioning Data Science Intern, Mahindra Group I interned at Mahindra Group to harness Artificial Intelligence for Real Estate business development.</description></item></channel></rss>